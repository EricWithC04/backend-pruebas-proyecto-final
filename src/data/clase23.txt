# Evaluación de Modelos de Aprendizaje Automático

## Introducción

Evaluar correctamente los modelos de aprendizaje automático es crucial para asegurar que estén funcionando de manera efectiva y eficiente. En esta clase, exploraremos las principales métricas y técnicas utilizadas para evaluar los modelos de aprendizaje automático, tanto para problemas de clasificación como de regresión.

## Importancia de la Evaluación

La evaluación permite medir el rendimiento del modelo y compararlo con otros modelos. Además, ayuda a identificar posibles áreas de mejora y garantizar que el modelo generalice bien a datos no vistos.

## Conjunto de Datos de Evaluación

Dividir el conjunto de datos en conjuntos de entrenamiento y prueba es una práctica estándar para evaluar el rendimiento del modelo:

- **Conjunto de Entrenamiento:** Utilizado para entrenar el modelo.
- **Conjunto de Prueba:** Utilizado para evaluar el rendimiento del modelo.

Adicionalmente, se puede utilizar la validación cruzada para una evaluación más robusta.

## Evaluación de Modelos de Clasificación

### 1. Matriz de Confusión

Una matriz de confusión muestra el número de predicciones correctas e incorrectas desglosadas por clase.

- **Componentes:**
  - Verdaderos Positivos (TP)
  - Verdaderos Negativos (TN)
  - Falsos Positivos (FP)
  - Falsos Negativos (FN)

### 2. Exactitud (Accuracy)

La exactitud mide el porcentaje de predicciones correctas.

\[ \text{Exactitud} = \frac{TP + TN}{TP + TN + FP + FN} \]

### 3. Precisión (Precision)

La precisión mide la proporción de verdaderos positivos sobre el total de predicciones positivas.

\[ \text{Precisión} = \frac{TP}{TP + FP} \]

### 4. Recall (Sensibilidad)

El recall mide la proporción de verdaderos positivos sobre el total de positivos reales.

\[ \text{Recall} = \frac{TP}{TP + FN} \]

### 5. F1 Score

El F1 Score es la media armónica de la precisión y el recall.

\[ \text{F1 Score} = 2 \times \frac{\text{Precisión} \times \text{Recall}}{\text{Precisión} + \text{Recall}} \]

### 6. Curva ROC y AUC

La Curva ROC (Receiver Operating Characteristic) traza el recall frente a la tasa de falsos positivos. El AUC (Area Under the Curve) mide el área bajo la curva ROC.

## Evaluación de Modelos de Regresión

### 1. Error Absoluto Medio (MAE)

El MAE mide el promedio de los errores absolutos entre las predicciones y los valores reales.

\[ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y_i}| \]

### 2. Error Cuadrático Medio (MSE)

El MSE mide el promedio de los errores cuadrados entre las predicciones y los valores reales.

\[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]

### 3. Raíz del Error Cuadrático Medio (RMSE)

El RMSE es la raíz cuadrada del MSE.

\[ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2} \]

### 4. R^2 (Coeficiente de Determinación)

El R^2 mide la proporción de la varianza de la variable dependiente que es predecible a partir de la variable independiente.

\[ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \]

## Ejemplos Prácticos

### Ejemplo 1: Evaluación de un Modelo de Clasificación

#### Código

```python
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Ejemplo de datos
X = [[1], [2], [3], [4], [5]]
y = [0, 0, 1, 1, 1]

# Dividir los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Crear y entrenar el modelo
modelo = LogisticRegression()
modelo.fit(X_train, y_train)

# Predicciones
y_pred = modelo.predict(X_test)

# Evaluación
matriz_confusion = confusion_matrix(y_test, y_pred)
exactitud = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Matriz de Confusión:\n", matriz_confusion)
print("Exactitud:", exactitud)
print("Precisión:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC:", roc_auc)

```

### Ejemplo 2: Evaluación de un Modelo de Regresión

#### Código

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Ejemplo de datos
X = [[1], [2], [3], [4], [5]]
y = [1, 3, 3, 2, 5]

# Dividir los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Crear y entrenar el modelo
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Predicciones
y_pred = modelo.predict(X_test)

# Evaluación
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R^2:", r2)

```

## Conclusión

La evaluación de modelos de aprendizaje automático es un paso crucial para garantizar que los modelos funcionen de manera eficiente y efectiva. Utilizando las métricas y técnicas adecuadas, podemos medir el rendimiento del modelo, identificar áreas de mejora y asegurar que el modelo generalice bien a datos no vistos.

---

**Autor:** Eric Alejandro Mercado  
**Fecha:** 24 de Junio, 2024